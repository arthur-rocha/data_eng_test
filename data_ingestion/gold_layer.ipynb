{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fd339c2-8522-4dd5-b8c7-b6fc9bc5ba99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Camada Gold\n",
    "\n",
    "- Aqui criarei as tabelas de visao mais analitica e agregada, de facil consumo para as areas de negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcbf51d-4528-4759-8359-591b332f5bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, coalesce, lit, countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e1d99a-ff97-40d6-bb92-88606b8fe287",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volume e vendas por periodo"
    }
   },
   "outputs": [],
   "source": [
    "fato_sales = spark.table(\"ab_inbev.silver.fato_sales\")\n",
    "dim_date = spark.table(\"ab_inbev.silver.dim_date\")\n",
    "\n",
    "df = fato_sales.join(dim_date, fato_sales.DATE == dim_date.DATE, \"left\") \\\n",
    "    .groupBy(dim_date.YEAR, dim_date.MONTH, dim_date.PERIOD) \\\n",
    "    .agg(sum(fato_sales.Volume).alias(\"total_volume\"), count(\"*\").alias(\"sales_count\"))\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_periodo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932a6e74-54f1-45ca-ad86-c431e25343b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volume e vendas por brand no mês"
    }
   },
   "outputs": [],
   "source": [
    "dim_brand = spark.table(\"ab_inbev.silver.dim_brand\")\n",
    "dim_date = spark.table(\"ab_inbev.silver.dim_date\")\n",
    "fato_sales = spark.table(\"ab_inbev.silver.fato_sales\")\n",
    "\n",
    "#cross join para termos ate os registros de 0 vendas na tabela\n",
    "df = dim_brand.crossJoin(dim_date) \\\n",
    "    .join(fato_sales, (fato_sales.DATE == dim_date.DATE) & (fato_sales.CE_BRAND_FLVR == dim_brand.CE_BRAND_FLVR), \"left\") \\\n",
    "    .groupBy(dim_brand.BRAND_NM, dim_date.YEAR, dim_date.MONTH) \\\n",
    "    .agg(coalesce(sum(fato_sales.Volume), lit(0)).alias(\"total_volume\"), countDistinct(fato_sales.sale_id).alias(\"sales_count\"))\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_brand_mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa2f3a78-83fd-47bc-a7c6-c5a40545b4d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volume e vendas por trade group no mes"
    }
   },
   "outputs": [],
   "source": [
    "dim_channels = spark.table(\"ab_inbev.silver.dim_channels\")\n",
    "dim_date = spark.table(\"ab_inbev.silver.dim_date\")\n",
    "fato_sales = spark.table(\"ab_inbev.silver.fato_sales\")\n",
    "\n",
    "df = dim_channels.crossJoin(dim_date) \\\n",
    "    .join(fato_sales, (fato_sales.DATE == dim_date.DATE) & (fato_sales.TRADE_CHNL_DESC == dim_channels.TRADE_CHNL_DESC), \"left\") \\\n",
    "    .groupBy(dim_channels.TRADE_GROUP_DESC, dim_date.YEAR, dim_date.MONTH) \\\n",
    "    .agg(coalesce(sum(fato_sales.Volume), lit(0)).alias(\"total_volume\"), countDistinct(fato_sales.sale_id).alias(\"sales_count\"))\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_tradeGroup_mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4609dcb1-db64-451e-b903-56d645cfc604",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volume e vendas por pkg cat no mes"
    }
   },
   "outputs": [],
   "source": [
    "dim_pkg = spark.table(\"ab_inbev.silver.dim_pkg\")\n",
    "dim_date = spark.table(\"ab_inbev.silver.dim_date\")\n",
    "fato_sales = spark.table(\"ab_inbev.silver.fato_sales\")\n",
    "\n",
    "df = dim_pkg.crossJoin(dim_date) \\\n",
    "    .join(fato_sales, (fato_sales.DATE == dim_date.DATE) & (fato_sales.TSR_PCKG_NM == dim_pkg.TSR_PCKG_NM), \"left\") \\\n",
    "    .groupBy(dim_pkg.PKG_CAT, dim_date.YEAR, dim_date.MONTH) \\\n",
    "    .agg(coalesce(sum(fato_sales.Volume), lit(0)).alias(\"total_volume\"), countDistinct(fato_sales.sale_id).alias(\"sales_count\"))\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_pkg_mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d52feaa-5685-4cc3-afc7-a731c8e53662",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volumes e vendas por região"
    }
   },
   "outputs": [],
   "source": [
    "df = fato_sales.groupBy(\"Btlr_Org_LVL_C_Desc\") \\\n",
    "    .agg(sum(\"Volume\").alias(\"total_volume\"), count(\"*\").alias(\"sales_count\"))\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_regiao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acd5746-e90b-46f4-b14b-8233ac1a8b9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volumes e vendas por regiao e mes"
    }
   },
   "outputs": [],
   "source": [
    "fato_sales = spark.table(\"ab_inbev.silver.fato_sales\")\n",
    "dim_date = spark.table(\"ab_inbev.silver.dim_date\")\n",
    "\n",
    "df = fato_sales.join(dim_date, fato_sales.DATE == dim_date.DATE, \"left\") \\\n",
    "    .groupBy(fato_sales.Btlr_Org_LVL_C_Desc, dim_date.YEAR, dim_date.MONTH) \\\n",
    "    .agg(sum(fato_sales.Volume).alias(\"total_volume\"), count(\"*\").alias(\"sales_count\"))\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_regiao_mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e6160e-78cc-4702-978e-99fa29408f7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volume e vendas por regiao e trade groups"
    }
   },
   "outputs": [],
   "source": [
    "#me arrependi de nao ter feito uma dim de regiao\n",
    "df = spark.sql(\"\"\" \n",
    "    with dim_reg as (\n",
    "    select Btlr_Org_LVL_C_Desc\n",
    "    from ab_inbev.silver.fato_sales\n",
    "    group by 1\n",
    "    )\n",
    "\n",
    "    select b.TRADE_GROUP_DESC, d.Btlr_Org_LVL_C_Desc, coalesce(sum(f.Volume), 0) as total_volume, count(distinct f.sale_id) as sales_count\n",
    "    from ab_inbev.silver.dim_channels b\n",
    "    cross join dim_reg d\n",
    "    left join ab_inbev.silver.fato_sales f on f.Btlr_Org_LVL_C_Desc = d.Btlr_Org_LVL_C_Desc and f.TRADE_CHNL_DESC = b.TRADE_CHNL_DESC\n",
    "    group by 1,2\n",
    "\"\"\")\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_regiao_tradeGroup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be23ab33-0c18-438d-937b-dc71a3b0b338",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "volume e vendas por regiao e brands"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "with dim_reg as (\n",
    "  select Btlr_Org_LVL_C_Desc\n",
    "  from ab_inbev.silver.fato_sales\n",
    "  group by 1\n",
    ")\n",
    "\n",
    "select b.BRAND_NM, d.Btlr_Org_LVL_C_Desc, coalesce(sum(f.Volume), 0) as total_volume, count(distinct f.sale_id) as sales_count\n",
    "from ab_inbev.silver.dim_brand b\n",
    "cross join dim_reg d\n",
    "left join ab_inbev.silver.fato_sales f on f.Btlr_Org_LVL_C_Desc = d.Btlr_Org_LVL_C_Desc and f.CE_BRAND_FLVR = b.CE_BRAND_FLVR\n",
    "group by 1,2\n",
    "\"\"\")\n",
    "\n",
    "display(df)\n",
    "\n",
    "#escrita\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"ab_inbev.gold.tb_volume_regiao_brand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0e16faa-823a-48cc-ad89-cd5f64bc977e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### O que poderia ser melhorado nesse processo\n",
    "\n",
    "- Mais testes de integracao e data quality\n",
    "- Criar outras visoes importantes para o negocio\n",
    "- Criar um pipeline para dar merge com novos dados que estejam no blob container ou em outra fonte\n",
    "- Monitoria e logs\n",
    "- Schedular pipelines via workflows\n",
    "- Documentacao e governanca de dados (ex: colocar descricoes de colunas no catalogo do databricks)\n",
    "- Padronizacao em nomes de colunas, tabelas e variaveis"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
